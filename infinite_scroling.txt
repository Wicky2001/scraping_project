# import scrapy
# from scrapy_selenium import SeleniumRequest
# from selenium.webdriver.common.keys import Keys
# import time

# class ScrapingClubSpider(scrapy.Spider):
#     name = 'spider'
    
#     def start_requests(self):
#         start_url = 'https://sinhala.adaderana.lk/'
#         yield SeleniumRequest(url=start_url, callback=self.parse)

#     def parse(self, response):
       
#         # products = response.css(".post")
        
#         # for product in products:
#         #     url = product.css("a::attr(href)").get()
#         #     image = product.css(".card-img-top::attr(src)").get()
#         #     name = product.css("h4 a::text").get()
#         #     price = product.css("h5::text").get()
            
           
#             yield {
#                 "url": url,
#                 "image": image,
#                 "name": name,
#                 "price": price
#             }
        
    #     # Scroll down to load more products
    #     self.scroll_and_scrape(response)

    # def scroll_and_scrape(self, response):
    #     # Use Selenium to scroll the page and load more items
    #     driver = response.request.meta['driver']
        
    #     # Set the number of scrolls you want to perform
    #     scroll_count = 5
    #     last_height = driver.execute_script("return document.body.scrollHeight")
        
    #     for _ in range(scroll_count):
    #         # Scroll to the bottom of the page
    #         driver.find_element_by_tag_name('body').send_keys(Keys.END)
    #         time.sleep(3)  # wait for the page to load more products

    #         # Check the new height to see if we've reached the bottom
    #         new_height = driver.execute_script("return document.body.scrollHeight")
    #         if new_height == last_height:
    #             break
    #         last_height = new_height
        
    #     # After scrolling, scrape the newly loaded products
    #     # Make another request to the same URL to get the updated page with more products
    #     yield SeleniumRequest(
    #         url=response.url,
    #         callback=self.parse,
    #         meta={'driver': driver}  # Pass the driver object for continued scraping
    #     )
